diff --git a/aten/src/ATen/Context.cpp b/aten/src/ATen/Context.cpp
index 1ec545dfc0..e1b3cc7a24 100644
--- a/aten/src/ATen/Context.cpp
+++ b/aten/src/ATen/Context.cpp
@@ -25,6 +25,14 @@ Context& globalContext() {
   return globalContext_;
 }
 
+bool Context::extraOptimization() const {
+  return extra_optimization;
+}
+
+void Context::setExtraOptimization(bool b) {
+  extra_optimization = b;
+}
+
 // NB: This method is *purely* whether or not a user requested
 // that CuDNN was enabled, it doesn't actually say anything about
 // whether or not CuDNN is actually usable.
diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index 8816fb0872..60a000f9db 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -264,7 +264,12 @@ class TORCH_API Context {
   void setDefaultMobileCPUAllocator();
   void unsetDefaultMobileCPUAllocator();
 
+  bool extraOptimization() const;
+  void setExtraOptimization(bool);
+
  private:
+  bool extra_optimization =
+      c10::utils::check_env("TORCH_EXTRA_OPT") == true ? true : false;
   void initCUDAIfNeeded(DeviceType p) {
     if (p == DeviceType::CUDA) {
       lazyInitCUDA();
diff --git a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
index 337ddb546f..d0d062525c 100644
--- a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
+++ b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
@@ -5,6 +5,7 @@
 #include <iterator>
 #include <numeric>
 
+#include <ATen/Context.h>
 #include <ATen/Dispatch.h>
 #include <ATen/Parallel.h>
 #include <ATen/TensorIterator.h>
@@ -112,6 +113,29 @@ inline void _vec_softmax_lastdim(
     int64_t dim_size) {
   using Vec = vec::Vectorized<scalar_t>;
   int64_t grain_size = internal::GRAIN_SIZE / (16 * dim_size);
+
+  // Fast path: padding to help with vectorization
+  if (at::globalContext().extraOptimization() && dim_size < Vec::size()) {
+    const Vec min_vec = Vec(-std::numeric_limits<float>::infinity());
+    parallel_for(0, outer_size, grain_size, [&](int64_t begin, int64_t end) {
+      for (const auto i : c10::irange(begin, end)) {
+        scalar_t* input_data = input_data_base + i * dim_size;
+        scalar_t* output_data = output_data_base + i * dim_size;
+        Vec xvec = Vec::loadu(input_data, dim_size);      // (x1, x2, ..., 0, 0)
+        Vec tmp_vec = Vec::set(min_vec, xvec, dim_size);  // (x1, x2, ..., -inf, -inf)
+        scalar_t max_value = vec_reduce_all(
+            [](Vec& x, Vec& y) { return vec::maximum(x, y); }, tmp_vec);
+        tmp_vec = (tmp_vec - Vec(max_value)).exp(); // (exp(x1-m), exp(x2-m), ..., exp(-inf), exp(-inf))
+        xvec = Vec::set(xvec, tmp_vec, dim_size);   // (exp(x1-m), exp(x2-m), ..., 0, 0)
+        scalar_t sum_value = vec::vec_reduce_all([](Vec& x, Vec& y) { return x + y; }, xvec);
+        sum_value = 1 / sum_value;
+        xvec = xvec * Vec(sum_value);
+        xvec.store(output_data, dim_size);
+      }
+    });
+    return;
+  }
+
   parallel_for(0, outer_size, grain_size, [&](int64_t begin, int64_t end) {
     for (const auto i : c10::irange(begin, end)) {
       scalar_t* input_data = input_data_base + i * dim_size;
diff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py
index f0c152adbc..1e0d6a79d7 100644
--- a/test/inductor/test_torchinductor.py
+++ b/test/inductor/test_torchinductor.py
@@ -95,6 +95,7 @@ slow = functools.partial(unittest.skipIf, not TEST_WITH_SLOW, "too slow")
 skip_if_x86_mac = functools.partial(
     unittest.skipIf, IS_MACOS and IS_X86, "Does not work on x86 Mac"
 )
+vec_dtypes = [torch.float, torch.bfloat16]
 
 # For OneDNN bf16 path, OneDNN requires the cpu has intel avx512 with avx512bw,
 # avx512vl, and avx512dq at least. So we will skip the test case if one processor
@@ -6006,17 +6007,20 @@ if HAS_CPU:
             def fn(x):
                 return (torch.log1p(torch.expm1(torch.erf(x))),)
 
-            x = torch.randn((2, 9))
-            x[0, 0] = torch.nan
-            x[1, -1] = torch.nan
+            for dtype in vec_dtypes:
+                x = torch.randn((2, 9), dtype=dtype)
+                x[0, 0] = torch.nan
+                x[1, -1] = torch.nan
 
-            with config.patch({"cpp.simdlen": None}):
-                torch._dynamo.reset()
-                metrics.reset()
-                traced = make_fx(fn)(x)
-                compiled = compile_fx_inner(traced, [x])
-                assert same(fn(x)[0], compiled([x])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                tol = 1e-2 if dtype == torch.bfloat16 else 1e-4
+
+                with config.patch({"cpp.simdlen": None}):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    traced = make_fx(fn)(x)
+                    compiled = compile_fx_inner(traced, [x])
+                    assert same(fn(x)[0], compiled([x])[0], equal_nan=True, tol=tol)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
 
         @unittest.skipIf(
             not codecache.valid_vec_isa_list(), "Does not support vectorization"
@@ -6074,19 +6078,20 @@ if HAS_CPU:
                 x = torch.where(y12, x, x + 1.0)
                 return (x,)
 
-            x = torch.randn((2, 9))
+            for dtype in vec_dtypes:
+                x = torch.randn((2, 9), dtype=dtype)
 
-            with config.patch({"cpp.simdlen": None}):
-                torch._dynamo.reset()
-                metrics.reset()
-                traced = make_fx(fn)(x)
-                compiled = compile_fx_inner(traced, [x])
-                assert same(fn(x)[0], compiled([x])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
-                assert (
-                    metrics.generated_kernel_count
-                    - metrics.generated_cpp_vec_kernel_count
-                ) == 0
+                with config.patch({"cpp.simdlen": None}):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    traced = make_fx(fn)(x)
+                    compiled = compile_fx_inner(traced, [x])
+                    assert same(fn(x)[0], compiled([x])[0], equal_nan=True)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
+                    assert (
+                        metrics.generated_kernel_count
+                        - metrics.generated_cpp_vec_kernel_count
+                    ) == 0
 
         @unittest.skipIf(
             not codecache.valid_vec_isa_list(), "Does not support vectorization"
@@ -6286,19 +6291,21 @@ if HAS_CPU:
         )
         @patch("torch.cuda.is_available", lambda: False)
         def test_maxpool2d_cpu_only(self):
-            input = torch.randn(10, 32, 20, 20).to(memory_format=torch.channels_last)
-            maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+            for dtype in vec_dtypes:
+                input = torch.randn(10, 32, 20, 20, dtype=dtype).to(memory_format=torch.channels_last)
+                maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
 
-            def func(x):
-                return maxpool(x)
+                def func(x):
+                    return maxpool(x)
 
-            with patch.object(config.cpp, "simdlen", None):
-                torch._dynamo.reset()
-                metrics.reset()
-                graph = torch.compile(func, backend="inductor")
-                graph(input)
-                assert same(graph(input), func(input), equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                with patch.object(config.cpp, "simdlen", None):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    graph = torch.compile(func, backend="inductor")
+                    graph(input)
+                    assert same(graph(input), func(input), equal_nan=True)
+                    # bfloat16 not support vectorization yet
+                    assert metrics.generated_cpp_vec_kernel_count == 1 or dtype == torch.bfloat16
 
         @unittest.skipIf(
             not codecache.valid_vec_isa_list(), "Does not support vectorization"
@@ -6307,18 +6314,19 @@ if HAS_CPU:
         def test_sign_cpu_only(self):
             def fn(x):
                 return (torch.sign(x),)
+            
+            for dtype in vec_dtypes:
+                x = torch.randn((2, 9), dtype=dtype)
+                x[0, 0] = torch.nan
+                x[1, -1] = torch.nan
 
-            x = torch.randn((2, 9))
-            x[0, 0] = torch.nan
-            x[1, -1] = torch.nan
-
-            with config.patch({"cpp.simdlen": None}):
-                torch._dynamo.reset()
-                metrics.reset()
-                traced = make_fx(fn)(x)
-                compiled = compile_fx_inner(traced, [x])
-                assert same(fn(x)[0], compiled([x])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                with config.patch({"cpp.simdlen": None}):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    traced = make_fx(fn)(x)
+                    compiled = compile_fx_inner(traced, [x])
+                    assert same(fn(x)[0], compiled([x])[0], equal_nan=True)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
 
         # Currently, we enabled AVX2 and AVX512 for vectorization. If the platform is not
         # supported, the vectorization will not work and skip this test case. For ARM or
@@ -6361,42 +6369,45 @@ if HAS_CPU:
                 res = x + x2
                 return (res,)
 
-            x1 = torch.randn((10, 20))
-            x2 = torch.randn((10, 20))
-
-            with config.patch({"cpp.simdlen": 1}):
-                torch._dynamo.reset()
-                metrics.reset()
-                traced = make_fx(fn)(x1, x2)
-                compiled = compile_fx_inner(traced, [x1, x2])
-                assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 0
+            for dtype in vec_dtypes:
+                torch.manual_seed(0)
+                x1 = torch.randn((5, 20), dtype=dtype)
+                x2 = torch.randn((5, 20), dtype=dtype)
+                tol = 1e-2 if dtype == torch.bfloat16 else 1e-4
+                
+                with config.patch({"cpp.simdlen": 1}):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    traced = make_fx(fn)(x1, x2)
+                    compiled = compile_fx_inner(traced, [x1, x2])
+                    assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True, tol=tol)
+                    assert metrics.generated_cpp_vec_kernel_count == 0
 
-            with config.patch({"cpp.simdlen": None}):
-                torch._dynamo.reset()
-                metrics.reset()
-                traced = make_fx(fn)(x1, x2)
-                compiled = compile_fx_inner(traced, [x1, x2])
-                assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                with config.patch({"cpp.simdlen": None}):
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    traced = make_fx(fn)(x1, x2)
+                    compiled = compile_fx_inner(traced, [x1, x2])
+                    assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
 
-                torch._dynamo.reset()
-                metrics.reset()
-                x1 = x1.permute(1, 0)
-                x2 = torch.randn((20, 10))
-                traced = make_fx(fn)(x1, x2)
-                compiled = compile_fx_inner(traced, [x1, x2])
-                assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    x1 = torch.randn(10, 20).permute(1, 0)
+                    x2 = torch.randn((20, 10))
+                    traced = make_fx(fn)(x1, x2)
+                    compiled = compile_fx_inner(traced, [x1, x2])
+                    assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
 
-                torch._dynamo.reset()
-                metrics.reset()
-                x1 = torch.randn((10, 7))
-                x2 = torch.randn((10, 7))
-                traced = make_fx(fn)(x1, x2)
-                compiled = compile_fx_inner(traced, ([x1, x2]))
-                assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
-                assert metrics.generated_cpp_vec_kernel_count == 1
+                    torch._dynamo.reset()
+                    metrics.reset()
+                    x1 = torch.randn((10, 7))
+                    x2 = torch.randn((10, 7))
+                    traced = make_fx(fn)(x1, x2)
+                    compiled = compile_fx_inner(traced, ([x1, x2]))
+                    assert same(fn(x1, x2)[0], compiled([x1, x2])[0], equal_nan=True)
+                    assert metrics.generated_cpp_vec_kernel_count == 1
 
         @unittest.skipIf(
             sys.platform != "linux", "cpp kernel profile only support linux now"
diff --git a/torch/_C/__init__.pyi.in b/torch/_C/__init__.pyi.in
index 2fef4aaf8e..315d6b929b 100644
--- a/torch/_C/__init__.pyi.in
+++ b/torch/_C/__init__.pyi.in
@@ -862,6 +862,8 @@ def _get_cublas_allow_tf32() -> _bool: ...  # THPModule_allowTF32CuBLAS
 def _set_cublas_allow_tf32(arg: _bool) -> None: ...  # THPModule_setAllowTF32CuBLAS
 def _get_float32_matmul_precision() -> str: ... #THPModule_float32MatmulPrecision
 def _set_float32_matmul_precision(arg: str) -> None: ... #THPModule_setFloat32MatmulPrecision
+def _set_extra_optimization(args: _bool) -> None: ... #THPModule_setExtraOptimization
+def _extra_optimization() -> _bool: ... #THPModule_extraOptimization
 def _get_cublas_allow_fp16_reduced_precision_reduction() -> _bool: ... #THPModule_allowFP16ReductionCuBLAS
 def _set_cublas_allow_fp16_reduced_precision_reduction(arg: _bool) -> None: ... #THPModule_setAllowFP16ReductionCuBLAS
 def _get_cublas_allow_bf16_reduced_precision_reduction() -> _bool: ... #THPModule_allowBF16ReductionCuBLAS
diff --git a/torch/__init__.py b/torch/__init__.py
index 77c24a5b59..71689f8b0a 100644
--- a/torch/__init__.py
+++ b/torch/__init__.py
@@ -899,6 +899,22 @@ def set_float32_matmul_precision(precision):
     """
     _C._set_float32_matmul_precision(precision)
 
+def set_extra_optimization(flag):
+    r"""Sets the extra optimization flag.
+
+    Args:
+        flag(bool): If True, enables extra optimizations.
+    """
+    _C._set_extra_optimization(flag)
+
+def get_extra_optimization():
+    r"""Returns the extra optimization flag.
+
+    Returns:
+        bool: The extra optimization flag.
+    """
+    return _C._extra_optimization()
+
 def set_warn_always(b):
     r"""When this flag is False (default) then some PyTorch warnings may only
     appear once per process. This helps avoid excessive warning information.
diff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py
index de6a32421c..bbd9b5da9e 100644
--- a/torch/_inductor/codegen/cpp.py
+++ b/torch/_inductor/codegen/cpp.py
@@ -15,6 +15,7 @@ import torch.fx
 from torch._prims_common import is_float_dtype
 
 from .. import codecache, config, ir, metrics
+from ..scheduler import SchedulerNode
 from ..codegen.wrapper import WrapperCodeGen
 from ..utils import cache_on_self, sympy_product, sympy_subs, sympy_symbol
 from ..virtualized import ops, V
@@ -199,6 +200,10 @@ class OptimizationContext:
     is_masked_load: bool = False
     # Load value as mask
     is_load_as_mask: bool = False
+    # Load bfloat16 value as float32
+    is_load_bf16_as_fp32: bool = False
+    # Store float32 value as bfloat16
+    is_store_fp32_as_bf16: bool = False
 
     dtype: torch.dtype = torch.float
     ops_name: str = ""
@@ -447,6 +452,8 @@ class CppVecOverrides(OpOverrides):
         opt_ctx: OptimizationContext = get_current_node_opt_ctx()
         assert opt_ctx
         assert opt_ctx.dtype in [torch.int32, torch.float32]
+        if dtype in [torch.bfloat16]:
+            assert opt_ctx.is_load_bf16_as_fp32
         proposed_dtype = opt_ctx.dtype
         if val == float("inf"):
             assert proposed_dtype == torch.float
@@ -525,7 +532,11 @@ class CppVecOverrides(OpOverrides):
 
     @staticmethod
     def to_dtype(x, dtype):
-        assert dtype in [torch.bool], f"{__name__} does not support {dtype}"
+        assert dtype in [
+            torch.bool,
+            torch.bfloat16,
+            torch.float,
+        ], f"{__name__} does not support {dtype}"
         return f"({x})"
 
     @staticmethod
@@ -870,7 +881,7 @@ class CppKernel(Kernel):
         var = self.args.input(name)
         index = self.rename_indexing(index)
         line = f"{var}[{cexpr(index)}]"
-        if V.graph.get_dtype(name) in (torch.float16, torch.bfloat16):
+        if V.graph.get_dtype(name) in [torch.float16]:
             line = f"static_cast<float>({line})"
         return self.cse.generate(self.loads, line)
 
@@ -1112,6 +1123,8 @@ class CppVecKernel(CppKernel):
                 f"flag_to_float({var_expr}, {self.var_vec_buf_map[var]}, {nelements});"
             )
             line = f"at::vec::Vectorized<float>::loadu({self.var_vec_buf_map[var]})"
+        elif V.graph.get_dtype(name) in [torch.bfloat16]:
+            line = f"load_bf16_as_float({var_expr})"
         elif is_broadcast:
             line = f"at::vec::Vectorized<float>({var_expr})"
         else:
@@ -1128,7 +1141,10 @@ class CppVecKernel(CppKernel):
         expanded_index = sympy.expand(index)
         new_index = self.scale_index_with_offset(index, self.tiling_factor)
         assert new_index != expanded_index
-        line = f"{value}.store({var} + {cexpr(new_index)});"
+        if V.graph.get_dtype(name) in [torch.bfloat16]:
+            line = f"store_float_as_bf16({var} + {cexpr(new_index)}, {value});"
+        else:
+            line = f"{value}.store({var} + {cexpr(new_index)});"
         self.stores.writeline(name, line)
 
     def reduction(self, name, dtype, src_dtype, reduction_type, index, value):
@@ -1394,11 +1410,11 @@ class CppVecKernelChecker(CppVecKernel):
         self.load_results: list[CSEVariable] = []
         self.load_supported_dtypes: list[torch.dtype] = [
             torch.float,
-            torch.float32,
+            torch.bfloat16,
             torch.bool,
             torch.uint8,
         ]
-        self.store_supported_dtypes: list[torch.dtype] = [torch.float, torch.float32]
+        self.store_supported_dtypes: list[torch.dtype] = [torch.float, torch.bfloat16]
         # Cache the dtypes of the store operation. If the store is mixing dtypes, the
         # vectorization would not support it as it is hard to determine the vec dtype
         self.store_dtypes: list[torch.dtype] = []
@@ -1452,6 +1468,34 @@ class CppVecKernelChecker(CppVecKernel):
         else:
             return False
 
+    def can_load_bf16_as_fp32(self, input_node: torch.fx.Node):
+        assert input_node.target in ["load", "constant"]
+        load_type = (
+            V.graph.get_dtype(input_node.args[1])
+            if input_node.target == "load"
+            else input_node.args[-1]
+        )
+        if load_type not in [torch.bfloat16]:
+            return False
+
+        if not all(
+            user.target == "to_dtype" and user.args[-1] == torch.float
+            for user in input_node.users
+        ):
+            return False
+
+        return True
+
+    def can_store_fp32_as_bf16(self, store_var: str, value_node: torch.fx.Node):
+        load_type = V.graph.get_dtype(store_var)
+        if load_type not in [torch.bfloat16]:
+            return False
+
+        if value_node.target == "to_dtype" and value_node.args[-1] == torch.bfloat16:
+            return True
+
+        return False
+
     def load(self, name: str, index: sympy.Expr):
         with RecordOptimizationContext(__name__) as node_ctx:
             load_dtype = V.graph.get_dtype(name)
@@ -1471,6 +1515,14 @@ class CppVecKernelChecker(CppVecKernel):
                 self.simd_vec = False
                 return var
 
+            if load_dtype in [torch.bfloat16]:
+                opt_ctx.is_load_bf16_as_fp32 = self.can_load_bf16_as_fp32(
+                    node_ctx.get_fx_node()
+                )
+                if not opt_ctx.is_load_bf16_as_fp32:
+                    self.simd_vec = False
+                    return var
+
             index = self.rename_indexing(index)
             self.simd_vec = self.simd_vec and self.could_vec(name, index)
             return var
@@ -1489,6 +1541,15 @@ class CppVecKernelChecker(CppVecKernel):
                 self.simd_vec = False
                 return self.simd_vec
 
+            if store_dtype in [torch.bfloat16]:
+                value_node = node_ctx.get_fx_node().all_input_nodes[-1]
+                opt_ctx.is_store_fp32_as_bf16 = self.can_store_fp32_as_bf16(
+                    name, value_node
+                )
+                if not opt_ctx.is_store_fp32_as_bf16:
+                    self.simd_vec = False
+                    return self.simd_vec
+
             assert "buf" in name
             index = self.rename_indexing(index)
 
@@ -1537,7 +1598,7 @@ class CppVecKernelChecker(CppVecKernel):
             return left_dtype == right_dtype
 
     def is_load_only_block(self, sub_graph: torch.fx.Graph):
-        # The sub graph only contains "placeholder", "output", "get_index", "load"
+        # The sub graph only contains "placeholder", "output", "get_index", "load",  "to_dtype"
         is_load_only = False
         load_dtype = None
         skip_io_nodes = ["placeholder", "output"]
@@ -1545,7 +1606,7 @@ class CppVecKernelChecker(CppVecKernel):
             if _node.op in skip_io_nodes:
                 continue
 
-            if _node.target not in ["load", "get_index"]:
+            if _node.target not in ["load", "get_index", "to_dtype"]:
                 # The body contains non load node
                 is_load_only = False
                 break
@@ -1554,6 +1615,28 @@ class CppVecKernelChecker(CppVecKernel):
                 _, name, _ = _node.args
                 load_dtype = V.graph.get_dtype(name)
                 is_load_only = True
+            
+            if _node.target == "to_dtype":
+                dtype = _node.args[-1]
+                if dtype == torch.float:
+                    # load -> to_dtype
+                    is_load_only = all(
+                        usr.target in ["ops", "load", "constant"]
+                        for usr in _node.all_input_nodes
+                    )
+                elif dtype == torch.bfloat16:
+                    # to_dtype -> store
+                    is_load_only = all(usr.target in ["store"] for usr in _node.users)
+                else:
+                    is_load_only = False
+
+                if not is_load_only:
+                    break
+            
+            if _node.target == "get_index":
+                is_load_only = all(usr.target in ["load"] for usr in _node.users)
+                if not is_load_only:
+                    break
 
         return is_load_only, load_dtype
 
@@ -1632,11 +1715,17 @@ class CppVecKernelChecker(CppVecKernel):
                         ):
                             opt_ctx.dtype = torch.float32
 
-                    supported_dtype = (torch.float32, torch.int32)
-                    is_supported_dtype = opt_ctx.dtype in (supported_dtype)
-                    if not is_supported_dtype:
+                    supported_dtypes = [torch.float32, torch.int32, torch.bfloat16]
+                    if opt_ctx.dtype not in supported_dtypes:
                         self.simd_vec = False
-                    return is_supported_dtype
+                    if opt_ctx.dtype in [torch.bfloat16]:
+                        if self.can_load_bf16_as_fp32(node_ctx.get_fx_node()):
+                            opt_ctx.is_load_bf16_as_fp32 = True
+                            opt_ctx.dtype = torch.float
+                        else:
+                            self.simd_vec = False
+
+                    return val
 
             @staticmethod
             def index_expr(expr, dtype):
@@ -1737,7 +1826,43 @@ class CppVecKernelChecker(CppVecKernel):
                     assert opt_ctx
                     opt_ctx.dtype = dtype
 
-                    if dtype != torch.bool:
+                    cur_node = node_ctx.get_fx_node()
+                    input_value: torch.fx.Node = cur_node.all_input_nodes[1]
+                    if dtype == torch.float and input_value.target in [
+                        "load",
+                        "constant",
+                    ]:
+                        # Support masked_load for BF16. Because the legalization will
+                        # insert to_dtype to convert the BF16 input to FP32.
+                        dtype = (
+                            V.graph.get_dtype(input_value.args[1])
+                            if input_value.target == "load"
+                            else input_value.args[-1]
+                        )
+                        if dtype in [torch.bfloat16]:
+                            opt_ctx.is_load_bf16_as_fp32 = True
+                        else:
+                            self.simd_vec = False
+                    elif dtype == torch.float and input_value.target in ["where"]:
+                        # Support masked_fill_softmax
+                        pass
+                    elif dtype == torch.bfloat16:
+                        if not all(usr.target == "store" for usr in cur_node.users):
+                            self.simd_vec = False
+                            return x
+
+                        store_names = [usr.args[1] for usr in cur_node.users]
+                        if not all(
+                            V.graph.get_dtype(name) in [torch.bfloat16]
+                            for name in store_names
+                        ):
+                            self.simd_vec = False
+                            return x
+
+                        opt_ctx.is_store_fp32_as_bf16 = True
+                    elif dtype == torch.bool:
+                        pass
+                    else:
                         self.simd_vec = False
                     return x
 
@@ -1828,7 +1953,96 @@ class CppKernelProxy(CppKernel):
         self.call_ranges = None
         self.picked_vec_isa: codecache.VecISA = codecache.pick_vec_isa()
 
+    def legalize_bf16(self, nodes):
+        def add_to_dtype(sub_graph: torch.fx.Graph):
+            for node in sub_graph.nodes:
+                _node: torch.fx.Node = node
+                if _node.target in ["load", "constant"]:
+                    assert len(_node.args) == 3
+                    ops = _node.args[0]
+                    # If the node is constant, the last arg is dtype
+                    load_dtype = (
+                        V.graph.get_dtype(_node.args[1])
+                        if _node.target == "load"
+                        else _node.args[-1]
+                    )
+
+                    if load_dtype == torch.bfloat16:
+                        with sub_graph.inserting_after(_node):
+                            to_type_node = sub_graph.call_method(
+                                "to_dtype", args=(ops, _node, torch.float)
+                            )
+                            to_type_node_args = to_type_node.args
+                            _node.replace_all_uses_with(to_type_node)
+                            to_type_node.args = to_type_node_args
+                elif _node.target == "store":
+                    ops, store_var, _, value_var, _ = _node.args
+                    store_dtype = V.graph.get_dtype(store_var)
+                    if store_dtype == torch.bfloat16:
+                        with sub_graph.inserting_before(_node):
+                            to_type_node = sub_graph.call_method(
+                                "to_dtype", args=(ops, value_var, torch.bfloat16)
+                            )
+                            _node.replace_input_with(value_var, to_type_node)
+                elif _node.target == "reduction":
+                    (
+                        ops,
+                        name,
+                        dtype,
+                        src_dtype,
+                        reduction_type,
+                        index,
+                        value,
+                    ) = _node.args
+                    if src_dtype == torch.bfloat16:
+                        # Since we always convert the load/store value to float if the tensor is blfoat16.
+                        # Therefore, the reduction should never work with bfloat16 value. Hence, we update
+                        # the bfloat16 reduction by updating the dtype and src_dtype to float.
+                        assert dtype in [torch.float, torch.bfloat16]
+                        _node.args = (
+                            ops,
+                            name,
+                            torch.float,
+                            torch.float,
+                            reduction_type,
+                            index,
+                            value,
+                        )
+                elif _node.target == "to_dtype" and _node.args[-1] in [torch.bfloat16]:
+                    (ops, x, _) = _node.args
+                    from_load = _node.all_input_nodes[-1].target == "load"
+                    to_store = all(usr.target == "store" for usr in _node.users)
+                    # The legalization always loads the BF16 tensor as FP32 for computation and converts
+                    # back to BF16 after the computation. Hence, there should be no computation w/ BF16.
+                    # Therefore, we update the to_dtype by replacing the bf16 dtype with fp32.
+                    if not (from_load or to_store):
+                        _node.args = (ops, x, torch.float)
+                else:
+                    pass
+
+            def eliminate_to_dtype(sub_graph: torch.fx.Graph):
+                # TODO(Eikan) Remove redundant to_dtype like load_bf16 + to_fp32 + to_bf16 + store_bf16
+                # => load_bf16 + store_bf16
+                pass
+
+            eliminate_to_dtype(sub_graph)
+
+        def _legalize_bf16(loop_body: ir.LoopBody):
+            sub_blocks = [loop_body.root_block] + list(loop_body.subblocks.values())
+            for sub_block in sub_blocks:
+                add_to_dtype(sub_block.graph)
+
+        for _node in nodes:
+            assert isinstance(_node, SchedulerNode)
+            node: SchedulerNode = _node
+            if isinstance(node._body, ir.LoopBody):
+                body: ir.LoopBody = node._body
+                _legalize_bf16(body)
+
     def codegen_nodes(self, nodes):
+        # Legalize BF16 node by adding to_dtype explicitly
+        self.legalize_bf16(nodes)
+
         kernel_group = self.kernel_group
         _, (group, reduction_group) = max(
             nodes, key=lambda x: int(x.is_reduction())
diff --git a/torch/_inductor/codegen/cpp_prefix.h b/torch/_inductor/codegen/cpp_prefix.h
index e0dba66314..a508dd902e 100644
--- a/torch/_inductor/codegen/cpp_prefix.h
+++ b/torch/_inductor/codegen/cpp_prefix.h
@@ -80,6 +80,20 @@ void flag_to_float(T src, float* dst, int64_t n) {
 }
 
 #if defined(CPU_CAPABILITY_AVX512) || defined(CPU_CAPABILITY_AVX2)
+
+inline at::vec::Vectorized<float> load_bf16_as_float(const bfloat16* bf16_buf) {
+  at::vec::Vectorized<float> res_vec(0);
+  at::vec::load_fp32_from_bf16(bf16_buf, res_vec);
+  return res_vec;
+}
+
+inline void store_float_as_bf16(
+    bfloat16* bf16_buf,
+    at::vec::Vectorized<float> src_buf) {
+  auto res = at::vec::convert_float_bfloat16(src_buf, src_buf);
+  res.store(bf16_buf, at::vec::Vectorized<float>::size());
+}
+
 template <typename SRC>
 inline at::vec::Vectorized<float> to_float_mask(at::vec::Vectorized<SRC>& src) {
   assert(
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index a5ef894e41..9b6167d480 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -946,6 +946,27 @@ PyObject* THPModule_unsetDefaultMobileCPUAllocator(
   END_HANDLE_TH_ERRORS
 }
 
+PyObject* THPModule_setExtraOptimization(
+    PyObject* _unused,
+    PyObject* arg) {
+  THPUtils_assert(
+      PyBool_Check(arg),
+      "set_extra_optimization expects a bool, "
+      "but got %s",
+      THPUtils_typename(arg));
+  at::globalContext().setExtraOptimization(arg == Py_True);
+  Py_RETURN_NONE;
+}
+
+PyObject* THPModule_extraOptimization(
+    PyObject* _unused,
+    PyObject* noargs) {
+  if (at::globalContext().extraOptimization())
+    Py_RETURN_TRUE;
+  else
+    Py_RETURN_FALSE;
+}
+
 static PyObject* THPModule_vmapmode_increment_nesting(
     PyObject* _unused,
     PyObject* arg) {
@@ -1191,6 +1212,14 @@ static PyMethodDef TorchMethods[] = {
      (PyCFunction)(void (*)(void))THPModule_has_torch_function_variadic,
      METH_FASTCALL,
      nullptr},
+    {"_set_extra_optimization",
+     THPModule_setExtraOptimization,
+     METH_O,
+     nullptr},
+    {"_extra_optimization",
+     THPModule_extraOptimization,
+     METH_NOARGS,
+     nullptr},
     {nullptr, nullptr, 0, nullptr}};
 
 void THCPStream_init(PyObject* module);
